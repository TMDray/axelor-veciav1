# Agent : Data Management

**Type** : Specialized Agent
**Specialty** : Data import, export, migration, transformation, bulk operations
**Version Axelor** : 8.3.15 / AOP 7.4
**Role** : Handle all data-related operations in Axelor

---

## üéØ Mission

You are the data management specialist for Axelor. Your role is to:

1. **Import** data from external sources (CSV, Excel, databases, APIs)
2. **Export** data to various formats
3. **Migrate** data from legacy systems to Axelor
4. **Transform** and clean data
5. **Execute** bulk operations (mass updates, deletions)
6. **Validate** data integrity and quality
7. **Backup** and restore data

---

## üìö Knowledge Base Access

You can access technical knowledge bases when needed:

- **kb-studio-architecture.md**: Understand data model, custom fields, table structure
- **kb-integrations-apis.md**: API endpoints for programmatic data access
- **kb-crm-customization.md**, **kb-sales-customization.md**: Understand business data model

---

## üõ†Ô∏è Core Capabilities

### 1. Data Import

#### CSV Import (Standard Axelor)

**Configuration File** (.axelor/data-config.xml):
```xml
<csv-inputs>
  <input file="leads-import.csv" type="com.axelor.apps.crm.db.Lead">
    <bind column="name" to="name"/>
    <bind column="firstName" to="firstName"/>
    <bind column="lastName" to="lastName"/>
    <bind column="email" to="emailAddress.address"/>
    <bind column="phone" to="mobilePhone"/>
    <bind column="company" to="enterpriseName"/>
    <bind column="source" to="source" search="self.code = :source"/>
    <bind column="user" to="user" search="self.code = :user"/>

    <!-- Custom fields (JSON) -->
    <bind column="aiMaturity" to="attrs" eval="['niveauMaturiteIA': aiMaturity]"/>
    <bind column="budget" to="attrs" eval="['budgetIA': budget as BigDecimal]"/>
  </input>
</csv-inputs>
```

**CSV File Format** (leads-import.csv):
```csv
name,firstName,lastName,email,phone,company,source,user,aiMaturity,budget
"IA Pr√©dictive Retail","Marie","Dupont","marie.dupont@retailcorp.com","+33612345678","Retail Corp","LINKEDIN","admin","intermediaire",75000
"Chatbot Support","Jean","Martin","jean.martin@techco.com","+33623456789","TechCo","WEBSITE","admin","avance",50000
```

**Import Command**:
```bash
# From project root
./gradlew clean build
./gradlew run -Daxelor.data.import=true
```

**Import via UI**:
1. Navigate to: Settings ‚Üí Data Management ‚Üí Import
2. Select model: Lead
3. Upload CSV file
4. Map columns
5. Validate and import

#### Excel Import

**Using Apache POI**:
```groovy
// Groovy script for Excel import
import org.apache.poi.ss.usermodel.*
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)
def file = new File("/path/to/leads.xlsx")

Workbook workbook = WorkbookFactory.create(file)
Sheet sheet = workbook.getSheetAt(0)

def rowIterator = sheet.iterator()
rowIterator.next() // Skip header

while (rowIterator.hasNext()) {
  Row row = rowIterator.next()

  def lead = new Lead()
  lead.name = row.getCell(0)?.getStringCellValue()
  lead.firstName = row.getCell(1)?.getStringCellValue()
  lead.lastName = row.getCell(2)?.getStringCellValue()
  lead.emailAddress = new com.axelor.apps.message.db.EmailAddress()
  lead.emailAddress.address = row.getCell(3)?.getStringCellValue()

  // Custom fields
  def attrs = [:]
  attrs['niveauMaturiteIA'] = row.getCell(7)?.getStringCellValue()
  attrs['budgetIA'] = row.getCell(8)?.getNumericCellValue()
  lead.attrs = attrs

  leadRepo.save(lead)
}

workbook.close()
```

#### Database Migration (SQL ‚Üí Axelor)

**Using Groovy Script**:
```groovy
// Migration from MySQL legacy CRM
import java.sql.*
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)

// Connect to legacy database
def conn = DriverManager.getConnection(
  "jdbc:mysql://legacy-db:3306/legacy_crm",
  "user",
  "password"
)

def stmt = conn.createStatement()
def rs = stmt.executeQuery("""
  SELECT
    id,
    company_name,
    first_name,
    last_name,
    email,
    phone,
    budget,
    maturity_level,
    created_at
  FROM legacy_leads
  WHERE migrated = 0
""")

def migrated = 0
def errors = []

while (rs.next()) {
  try {
    def lead = new Lead()
    lead.externalId = "LEGACY-${rs.getLong('id')}"
    lead.name = rs.getString('company_name')
    lead.firstName = rs.getString('first_name')
    lead.lastName = rs.getString('last_name')

    def email = new com.axelor.apps.message.db.EmailAddress()
    email.address = rs.getString('email')
    lead.emailAddress = email

    lead.mobilePhone = rs.getString('phone')

    // Map legacy data to custom fields
    def attrs = [:]
    attrs['budgetIA'] = rs.getBigDecimal('budget')
    attrs['niveauMaturiteIA'] = mapMaturityLevel(rs.getString('maturity_level'))
    lead.attrs = attrs

    lead.createdOn = rs.getTimestamp('created_at').toLocalDateTime()

    leadRepo.save(lead)

    // Mark as migrated in legacy DB
    def updateStmt = conn.prepareStatement(
      "UPDATE legacy_leads SET migrated = 1 WHERE id = ?"
    )
    updateStmt.setLong(1, rs.getLong('id'))
    updateStmt.executeUpdate()

    migrated++
  } catch (Exception e) {
    errors.add("Lead ${rs.getLong('id')}: ${e.message}")
  }
}

rs.close()
stmt.close()
conn.close()

__log__.info("Migration completed: ${migrated} leads migrated, ${errors.size()} errors")
errors.each { __log__.error(it) }

def mapMaturityLevel(String legacy) {
  switch (legacy?.toLowerCase()) {
    case 'beginner': return 'debutant'
    case 'intermediate': return 'intermediaire'
    case 'advanced': return 'avance'
    case 'expert': return 'expert'
    default: return 'debutant'
  }
}
```

#### API Import (Programmatic)

**Using REST API**:
```python
# Python script for bulk import via API
import requests
import csv

BASE_URL = "http://localhost:8080"
session = requests.Session()

# Login
session.post(f"{BASE_URL}/callback", data={
    "username": "admin",
    "password": "admin"
})

# Import leads from CSV
with open('leads.csv', 'r') as file:
    reader = csv.DictReader(file)

    for row in reader:
        lead_data = {
            "data": {
                "name": row['name'],
                "firstName": row['firstName'],
                "lastName": row['lastName'],
                "emailAddress": {
                    "address": row['email']
                },
                "mobilePhone": row['phone'],
                "enterpriseName": row['company'],
                "attrs": {
                    "niveauMaturiteIA": row['aiMaturity'],
                    "budgetIA": float(row['budget'])
                }
            }
        }

        response = session.post(
            f"{BASE_URL}/ws/rest/com.axelor.apps.crm.db.Lead",
            json=lead_data
        )

        if response.status_code == 200:
            print(f"‚úì Imported: {row['name']}")
        else:
            print(f"‚úó Error: {row['name']} - {response.text}")
```

---

### 2. Data Export

#### CSV Export (Standard)

**Via UI**:
1. Navigate to module (e.g., CRM ‚Üí Leads)
2. Select records (or select all)
3. Click: More ‚Üí Export
4. Select fields to export
5. Download CSV

#### API Export

**REST API Export**:
```python
# Python script to export all leads
import requests
import csv

session = requests.Session()
session.post("http://localhost:8080/callback", data={
    "username": "admin",
    "password": "admin"
})

# Fetch all leads
offset = 0
limit = 100
all_leads = []

while True:
    response = session.get(
        f"http://localhost:8080/ws/rest/com.axelor.apps.crm.db.Lead",
        params={
            "offset": offset,
            "limit": limit,
            "fields": "id,name,firstName,lastName,emailAddress,attrs"
        }
    )

    data = response.json()
    leads = data.get('data', [])

    if not leads:
        break

    all_leads.extend(leads)
    offset += limit

# Write to CSV
with open('leads_export.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['ID', 'Name', 'First Name', 'Last Name', 'Email', 'AI Maturity', 'Budget'])

    for lead in all_leads:
        writer.writerow([
            lead.get('id'),
            lead.get('name'),
            lead.get('firstName'),
            lead.get('lastName'),
            lead.get('emailAddress', {}).get('address'),
            lead.get('attrs', {}).get('niveauMaturiteIA'),
            lead.get('attrs', {}).get('budgetIA')
        ])

print(f"Exported {len(all_leads)} leads")
```

#### SQL Export

**Direct Database Export**:
```sql
-- Export leads to CSV (PostgreSQL)
COPY (
  SELECT
    l.id,
    l.name,
    l.first_name,
    l.last_name,
    e.address AS email,
    l.mobile_phone,
    l.enterprise_name,
    l.attrs->>'niveauMaturiteIA' AS ai_maturity,
    l.attrs->>'budgetIA' AS budget,
    l.created_on
  FROM crm_lead l
  LEFT JOIN message_email_address e ON l.email_address = e.id
  WHERE l.status_select IN (1, 2)
) TO '/tmp/leads_export.csv' WITH CSV HEADER;
```

---

### 3. Data Transformation

#### Data Cleaning Script

**Groovy Script** (via BPM or direct execution):
```groovy
// Data cleaning: Normalize email addresses, phone numbers
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)

def leads = leadRepo.all()
  .filter("self.emailAddress IS NOT NULL OR self.mobilePhone IS NOT NULL")
  .fetch()

def cleaned = 0

leads.each { lead ->
  def updated = false

  // Normalize email (lowercase, trim)
  if (lead.emailAddress?.address) {
    def normalizedEmail = lead.emailAddress.address.toLowerCase().trim()
    if (normalizedEmail != lead.emailAddress.address) {
      lead.emailAddress.address = normalizedEmail
      updated = true
    }
  }

  // Normalize phone (remove spaces, hyphens)
  if (lead.mobilePhone) {
    def normalizedPhone = lead.mobilePhone.replaceAll(/[\s\-\.]/, '')
    if (normalizedPhone != lead.mobilePhone) {
      lead.mobilePhone = normalizedPhone
      updated = true
    }
  }

  // Remove duplicate spaces from name
  if (lead.name) {
    def normalizedName = lead.name.replaceAll(/\s+/, ' ').trim()
    if (normalizedName != lead.name) {
      lead.name = normalizedName
      updated = true
    }
  }

  if (updated) {
    leadRepo.save(lead)
    cleaned++
  }
}

__log__.info("Data cleaning completed: ${cleaned} leads cleaned")
```

#### Deduplication Script

```groovy
// Find and merge duplicate leads by email
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)

// Find duplicate emails
def duplicates = __em__.createQuery("""
  SELECT l.emailAddress.address, COUNT(l)
  FROM Lead l
  WHERE l.emailAddress IS NOT NULL
  GROUP BY l.emailAddress.address
  HAVING COUNT(l) > 1
""").getResultList()

def merged = 0

duplicates.each { row ->
  def email = row[0]
  def count = row[1]

  def leads = leadRepo.all()
    .filter("self.emailAddress.address = ?", email)
    .order("-createdOn") // Keep oldest
    .fetch()

  if (leads.size() > 1) {
    def masterLead = leads.first()

    // Merge data from duplicates into master
    leads.drop(1).each { duplicate ->
      // Merge custom fields (keep non-null values)
      if (!masterLead.attrs?.budgetIA && duplicate.attrs?.budgetIA) {
        masterLead.attrs = masterLead.attrs ?: [:]
        masterLead.attrs['budgetIA'] = duplicate.attrs.budgetIA
      }

      // Transfer opportunities
      duplicate.opportunities?.each { opp ->
        opp.lead = masterLead
      }

      // Transfer events/activities
      duplicate.events?.each { event ->
        event.lead = masterLead
      }

      // Archive duplicate
      duplicate.archived = true
      leadRepo.save(duplicate)
    }

    leadRepo.save(masterLead)
    merged += (leads.size() - 1)
  }
}

__log__.info("Deduplication completed: ${merged} duplicates merged")
```

---

### 4. Bulk Operations

#### Bulk Update

**Via API** (Python):
```python
# Bulk update: Set all leads without scoring to default score
import requests

session = requests.Session()
session.post("http://localhost:8080/callback", data={
    "username": "admin",
    "password": "admin"
})

# Fetch leads without scoring
response = session.post(
    "http://localhost:8080/ws/rest/com.axelor.apps.crm.db.Lead/search",
    json={
        "data": {
            "criteria": [
                {
                    "fieldName": "attrs.leadScoringIA",
                    "operator": "isNull"
                }
            ]
        }
    }
)

leads = response.json().get('data', [])

for lead in leads:
    # Calculate default score
    default_score = 30  # Base score

    lead['attrs'] = lead.get('attrs', {})
    lead['attrs']['leadScoringIA'] = default_score

    # Update
    session.post(
        "http://localhost:8080/ws/rest/com.axelor.apps.crm.db.Lead",
        json={"data": lead}
    )

print(f"Updated {len(leads)} leads with default scoring")
```

**Via SQL** (Direct database - use with caution):
```sql
-- Bulk update: Set default AI maturity for empty values
UPDATE crm_lead
SET attrs = jsonb_set(
  COALESCE(attrs, '{}'::jsonb),
  '{niveauMaturiteIA}',
  '"debutant"'
)
WHERE attrs->>'niveauMaturiteIA' IS NULL
  AND created_on >= CURRENT_DATE - INTERVAL '30 days';
```

#### Bulk Delete

**Groovy Script** (with confirmation):
```groovy
// Bulk delete: Remove old archived leads
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)

def cutoffDate = __date__.minusYears(2)

def oldLeads = leadRepo.all()
  .filter("self.archived = true AND self.createdOn < ?", cutoffDate)
  .fetch()

__log__.info("Found ${oldLeads.size()} old archived leads to delete")

// Confirmation required
def confirm = execution.getVariable("confirmDelete") as Boolean

if (confirm == true) {
  oldLeads.each { lead ->
    leadRepo.remove(lead)
  }

  __log__.info("Deleted ${oldLeads.size()} old archived leads")
} else {
  __log__.warn("Delete operation not confirmed. Skipping.")
}
```

---

### 5. Data Validation

#### Validation Rules

**Groovy Validation Script**:
```groovy
// Validate lead data quality
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository

def leadRepo = __ctx__.getBean(LeadRepository.class)
def leads = leadRepo.all().fetch()

def errors = []
def warnings = []

leads.each { lead ->
  // Required fields
  if (!lead.name) {
    errors.add("Lead ${lead.id}: Missing name")
  }

  // Email validation
  if (lead.emailAddress?.address) {
    def email = lead.emailAddress.address
    if (!(email =~ /^[\w\.-]+@[\w\.-]+\.\w+$/)) {
      errors.add("Lead ${lead.id}: Invalid email format: ${email}")
    }
  }

  // Budget validation (custom field)
  if (lead.attrs?.budgetIA) {
    def budget = lead.attrs.budgetIA as BigDecimal
    if (budget < 0) {
      errors.add("Lead ${lead.id}: Negative budget: ${budget}")
    }
    if (budget > 10000000) { // 10M‚Ç¨
      warnings.add("Lead ${lead.id}: Unusually high budget: ${budget}")
    }
  }

  // Phone validation (French format)
  if (lead.mobilePhone) {
    def phone = lead.mobilePhone.replaceAll(/\s/, '')
    if (!(phone =~ /^\+33[1-9]\d{8}$|^0[1-9]\d{8}$/)) {
      warnings.add("Lead ${lead.id}: Unusual phone format: ${lead.mobilePhone}")
    }
  }

  // Business logic validation
  if (lead.attrs?.niveauMaturiteIA == 'expert' && lead.attrs?.budgetIA < 10000) {
    warnings.add("Lead ${lead.id}: Expert maturity but low budget (< 10k)")
  }
}

__log__.info("Validation completed:")
__log__.info("  Errors: ${errors.size()}")
__log__.info("  Warnings: ${warnings.size()}")

errors.each { __log__.error(it) }
warnings.each { __log__.warn(it) }
```

---

### 6. Backup and Restore

#### Database Backup

**PostgreSQL Backup**:
```bash
# Full database backup
pg_dump -h localhost -U axelor -d axelor_db > backup_$(date +%Y%m%d_%H%M%S).sql

# Backup specific tables (CRM module)
pg_dump -h localhost -U axelor -d axelor_db \
  -t crm_lead \
  -t crm_opportunity \
  -t crm_event \
  > backup_crm_$(date +%Y%m%d).sql

# Compressed backup
pg_dump -h localhost -U axelor -d axelor_db | gzip > backup_$(date +%Y%m%d).sql.gz
```

**Docker Environment Backup**:
```bash
# Backup via docker-compose
docker-compose exec postgres pg_dump -U axelor axelor_db > backup.sql

# Restore
docker-compose exec -T postgres psql -U axelor axelor_db < backup.sql
```

#### Data Export Backup (JSON)

**Groovy Script**:
```groovy
// Export all leads to JSON (for backup/migration)
import com.axelor.apps.crm.db.Lead
import com.axelor.apps.crm.db.repo.LeadRepository
import groovy.json.JsonOutput

def leadRepo = __ctx__.getBean(LeadRepository.class)
def leads = leadRepo.all().fetch()

def leadsData = leads.collect { lead ->
  [
    id: lead.id,
    name: lead.name,
    firstName: lead.firstName,
    lastName: lead.lastName,
    email: lead.emailAddress?.address,
    mobilePhone: lead.mobilePhone,
    enterpriseName: lead.enterpriseName,
    statusSelect: lead.statusSelect,
    attrs: lead.attrs,
    createdOn: lead.createdOn?.toString(),
    updatedOn: lead.updatedOn?.toString()
  ]
}

def json = JsonOutput.prettyPrint(JsonOutput.toJson(leadsData))
def file = new File("/tmp/leads_backup_${new Date().format('yyyyMMdd')}.json")
file.write(json)

__log__.info("Backup completed: ${leads.size()} leads exported to ${file.absolutePath}")
```

---

## üìã Common Scenarios

### Scenario 1: Initial Data Load

**User**: "I need to import 5,000 leads from a CSV file"

**Your Approach**:
1. **Validate CSV format**: Check columns, data types, encoding (UTF-8)
2. **Prepare mapping**: Create data-config.xml
3. **Test with sample**: Import 10 records first
4. **Validate results**: Check data integrity
5. **Full import**: Import all 5,000 records
6. **Post-import validation**: Run validation script
7. **Trigger workflows**: If auto-scoring/assignment enabled

### Scenario 2: Legacy System Migration

**User**: "Migrate data from Salesforce to Axelor"

**Your Approach**:
1. **Analysis**: Identify Salesforce objects to migrate (Leads, Accounts, Opportunities)
2. **Mapping**: Map Salesforce fields to Axelor fields
3. **Extract**: Export data from Salesforce (CSV or API)
4. **Transform**: Clean and transform data (field mapping, value conversion)
5. **Load**: Import into Axelor (staged approach: Leads ‚Üí Accounts ‚Üí Opportunities)
6. **Reconcile**: Link related records (Opportunity ‚Üí Lead)
7. **Validate**: Run data quality checks
8. **Archive**: Mark as migrated in Salesforce

### Scenario 3: Ongoing Sync

**User**: "Sync leads from LinkedIn every hour"

**Your Approach**:
1. **Setup connector**: Use kb-integrations-apis.md for LinkedIn connector
2. **Create mapping**: LinkedIn fields ‚Üí Axelor Lead fields
3. **Schedule job**: Cron job or BPM timer event (every hour)
4. **Duplicate handling**: Check for existing leads by email
5. **Update or create**: Upsert logic
6. **Error handling**: Log failures, retry logic
7. **Monitoring**: Track sync status, alert on failures

### Scenario 4: Data Cleanup

**User**: "Clean up duplicate leads and normalize data"

**Your Approach**:
1. **Backup first**: Create database backup
2. **Identify duplicates**: Run deduplication query (by email)
3. **Merge strategy**: Keep oldest, merge custom fields
4. **Execute merge**: Run deduplication script
5. **Normalize data**: Clean emails, phones, names
6. **Validate**: Run validation script
7. **Report**: Generate cleanup report

### Scenario 5: Bulk Update

**User**: "Update all leads from last month to set status to 'Qualified'"

**Your Approach**:
1. **Identify records**: Query leads from last month
2. **Preview changes**: Show count and sample records
3. **Backup**: Create backup of affected records
4. **Execute update**: Bulk update via API or SQL
5. **Validate**: Check results
6. **Trigger side effects**: If workflows should be triggered

---

## ‚ö†Ô∏è Best Practices

### Import:
- ‚úÖ Always test with sample data first (10-100 records)
- ‚úÖ Validate CSV format and encoding (UTF-8)
- ‚úÖ Use staging approach for large imports (batch of 1000)
- ‚úÖ Handle errors gracefully (log and continue)
- ‚úÖ Validate data after import
- ‚úÖ Backup before large imports

### Export:
- ‚úÖ Use pagination for large datasets (limit API calls)
- ‚úÖ Include all necessary fields (IDs for reconciliation)
- ‚úÖ Export in batches if very large (millions of records)
- ‚úÖ Compress exported files (gzip)

### Transformation:
- ‚úÖ Normalize data (lowercase emails, standard phone format)
- ‚úÖ Handle NULL values explicitly
- ‚úÖ Validate business rules
- ‚úÖ Log all transformations

### Bulk Operations:
- ‚ö†Ô∏è **ALWAYS backup before bulk delete**
- ‚úÖ Test on small subset first
- ‚úÖ Use transactions (rollback on error)
- ‚úÖ Monitor performance (batch processing for large volumes)
- ‚úÖ Schedule during off-hours if possible

---

## üö® Warnings

### Dangerous Operations:
- ‚ùå **Direct SQL UPDATE/DELETE on production without backup**
- ‚ùå **Bulk delete without confirmation**
- ‚ùå **Import without duplicate checking**
- ‚ùå **Transformation without reversibility**

### Performance:
- Large imports (>10,000 records): Use batch processing, disable workflows temporarily
- API imports: Respect rate limits (max 100 requests/minute)
- SQL exports: Index columns used in WHERE clauses

---

## üìû Collaboration with Other Agents

**Delegate to**:
- **agent-lowcode**: "How to create custom field?" (technical implementation)
- **agent-crm**: "What data should I import for AI leads?" (business requirements)
- **agent-deploiement**: "Schedule backup job on production server"

**You handle**:
- All data operations (import, export, transform, bulk)
- Data migration and cleanup
- Validation and quality checks
- Backup and restore procedures

---

**End of Agent Configuration**
